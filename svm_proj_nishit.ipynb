{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b98ab7-b6db-44fd-b725-c06b7c4d53a1",
   "metadata": {},
   "source": [
    "<font size=\"20\"><u><b>Support Vector Machines (SVM)</b></u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492cd9ff-6d83-4eed-8ae8-ae3b2ed77631",
   "metadata": {},
   "source": [
    "<font size=\"6\"><u><b>By Nishit Sonawane</b></u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfb6e2-d72c-4b22-8a8c-ba1ba80a5c31",
   "metadata": {},
   "source": [
    "## •<u>Introduction to Support Vector Machines</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baca16c-eb7b-4936-ac18-88c9d9752a39",
   "metadata": {},
   "source": [
    "### – Support Vector Machines (SVMs) are powerful machine learning models used for classification and regression tasks. They have gained significant popularity due to their ability to handle complex datasets and produce accurate results. SVMs are particularly effective when dealing with high-dimensional feature spaces.\n",
    "\n",
    "### – At its core, SVM is a binary classification algorithm that aims to find an optimal hyperplane that separates different classes in a dataset. The hyperplane is determined by maximizing the margin, which is the distance between the hyperplane and the nearest data points of each class. The data points that lie closest to the hyperplane are known as support vectors, hence the name \"Support Vector Machines.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074836c-3211-4346-9eda-951cd13ac4ef",
   "metadata": {},
   "source": [
    "## •<u>Linearly Separable vs Non-Separable Data</u>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68598f86-83e3-4147-84fb-63a44fc57965",
   "metadata": {},
   "source": [
    "### Since scikit-learn's breast cancer dataset consists of multiple features, it is not as simple to determine linear separability by visual inspection. Linear separability refers to the property of data points being separable by a straight line in a 2D space or a hyperplane in higher-dimensional spaces.\n",
    "### In the case of the breast cancer dataset, the features represent various characteristics of tumors, such as mean radius, mean texture, etc. These features are not inherently visualizable in a 2D space, making it challenging to determine linear separability through visual inspection alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd6c07-27b2-47cb-afbf-b45c5c22a19a",
   "metadata": {},
   "source": [
    "# •<u>The use of Support Vector Machines (SVM) for classification on the Breast Cancer Wisconsin (Diagnostic) dataset.</u>•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb307c0-4d8a-4049-a3b8-171a755ba2b9",
   "metadata": {},
   "source": [
    "#### •<u>Importing necessary libraries:</u> pandas, numpy and sklearn modules are imported for data manipulation, and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f9ab8e-46de-465d-abb8-a5ad70e6c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832f376-40da-4405-9a4f-9d7741033095",
   "metadata": {},
   "source": [
    "#### •<u>Loading and exploring the dataset:</u>   The Breast Cancer dataset is loaded using 'load_breast_cancer()' from 'sklearn.datasets'. The dataset's description is printed using 'print(cancer['DESCR'])'. The dataset features are stored in 'df_feat', and their information is displayed using 'df_feat.info()'. The target names are printed using 'cancer['target_names']'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c7de23-c4f7-4fa0-9df8-336042507f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e62009c-e584-455a-983f-1be42a17d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93176478-c2a0-4b9c-b099-31d8dc98f9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5234e79-dc4f-48c0-829d-b18a377c26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0d7f54-f0a9-4e85-a481-6b8de846304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad7b5e9-943c-4213-937e-3521bf512d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7319ac98-9cd1-44f7-982c-59320666a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 133.5 KB\n"
     ]
    }
   ],
   "source": [
    "df_feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b807c7a1-b208-4e3e-9567-dc5b09d1b566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94aadc-8909-45cd-843f-35d7b50ab9cf",
   "metadata": {},
   "source": [
    "#### •<u>Splitting the data into training and testing sets:</u> The dataset is divided into input features (X) and target labels (y). The 'train_test_split()' function from 'sklearn.model_selection' is used to split the data into training and testing sets, with 30% of the data reserved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e840d15a-b9b1-4d7f-b207-c11bdb75fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "265cc261-e976-417a-af5f-3d8ee09cc169",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_feat\n",
    "y = cancer['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d76dd-f33f-49f6-82cf-64e2ade58e82",
   "metadata": {},
   "source": [
    "## •<u>The Kernal Trick</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95687906-669c-43f3-a320-db0578c06008",
   "metadata": {},
   "source": [
    "### – The kernel trick is a concept in machine learning, particularly in support vector machines (SVMs), that allows us to efficiently compute the results of a potentially high-dimensional feature space without explicitly calculating the transformed feature vectors. It is a mathematical technique that makes SVMs applicable to non-linear classification problems.\n",
    "### – The SVC (Support Vector Classifier) model from scikit-learn is employed to perform classification on the breast cancer dataset. The SVC model has a parameter called kernel which determines the type of kernel function to be used.\n",
    "### – By default, the SVC model uses the Radial Basis Function (RBF) kernel, which is a popular choice for non-linear classification problems. The RBF kernel implicitly maps the input data into a higher-dimensional space, allowing the model to find non-linear decision boundaries. This mapping is performed using the kernel trick, which avoids the explicit computation of the transformed feature space.\n",
    "### – In the context of breast cancer classification, the RBF kernel is effective in capturing non-linear patterns and boundaries in the feature space defined by the dataset. Since breast cancer classification is often a complex and non-linear problem, using the RBF kernel can provide better separation between malignant and benign samples, resulting in improved classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff984c-5a0a-4d0e-af88-c22d1078c344",
   "metadata": {},
   "source": [
    "#### •<u>Building an SVM model:</u> An SVM model is created using SVC() from sklearn.svm. The model is trained on the training data using model.fit(X_train, y_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a02d861-643b-4a75-926c-e614b745ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1fb5619-63d4-4786-b99a-6a97ac556a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1117a29-5af4-426f-affb-8e442a5ade94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27a162-eff3-4a8d-9720-72c8609e2e50",
   "metadata": {},
   "source": [
    "#### •<u>Make predictions and evaluate the model:</u> The trained SVM model is used to make predictions on the test data using 'model.predict(X_test)'. The confusion matrix and classification report are printed using 'confusion_matrix()' and 'classification_report()' from 'sklearn.metrics', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7164916f-bd8e-4c72-8d63-01aa751c4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "585f5bc9-07be-4f21-a4c5-77754028f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc34791c-ba4d-4274-8d58-bc3520e6d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56  10]\n",
      " [  3 102]]\n",
      "__________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90        66\n",
      "           1       0.91      0.97      0.94       105\n",
      "\n",
      "    accuracy                           0.92       171\n",
      "   macro avg       0.93      0.91      0.92       171\n",
      "weighted avg       0.93      0.92      0.92       171\n",
      "\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(50*'_')\n",
    "print(classification_report(y_test, predictions))\n",
    "print(50*'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79267025-de51-43f2-863d-e1d80458f027",
   "metadata": {},
   "source": [
    "### •<u>Confusion Matrix Explained (pre tuning)</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a995fae-4e2d-4e37-936d-229362dc5d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <u>[1,1] True Negative (TN):</u> The model correctly predicted 56 samples as negative (benign tumors).\n",
    "#### <u>[1,2] False Positive (FP):</u> The model incorrectly predicted 10 samples as positive (malignant tumors) when they were actually negative (benign tumors).\n",
    "#### <u>[2,1] False Negative (FN):</u> The model incorrectly predicted 3 samples as negative (benign tumors) when they were actually positive (malignant tumors).\n",
    "#### <u>[2,2] True Positive (TP):</u> The model correctly predicted 102 samples as positive (malignant tumors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63fc67-4350-487c-8683-410b6feaa8e2",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd79547-22d0-43da-a7dc-8da6365b2ed9",
   "metadata": {},
   "source": [
    "## •<u>Understanding the Hyperparameters – Cost and Gamma</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffb3f4-a7c2-46c7-a325-6c8c23595d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In Support Vector Machines (SVM), the hyperparameters \"C\" and \"gamma\" play crucial roles in the model's performance and behavior. Understanding these hyperparameters is essential for effectively tuning SVMs for different datasets and problem domains. Here's an explanation of the cost (C) and gamma hyperparameters:\n",
    "### <u>1. Cost (C):</u>\n",
    "### <u>1.1:</u> The cost parameter, often denoted as \"C,\" controls the trade-off between maximizing the margin and minimizing the classification errors in SVM.\n",
    "### <u>1.2:</u> A smaller value of C allows for a wider margin but may result in more misclassifications. It emphasizes a simpler decision boundary with potentially more margin violations.\n",
    "### <u>1.3:</u> A larger value of C leads to a narrower margin but aims to minimize the number of misclassifications. It prioritizes a more complex decision boundary that attempts to fit the training data more precisely.\n",
    "### <u>1.4:</u> The choice of C depends on the problem's characteristics and the desired balance between overfitting (smaller C) and underfitting (larger C).\n",
    "### <u>2. Gamma:</u>\n",
    "### <u>2.1:</u> The gamma parameter, often denoted as \"gamma,\" determines the influence of each training example on the decision boundary.\n",
    "### <u>2.2:</u> A smaller gamma value implies a larger influence radius and results in a smoother decision boundary. It leads to a more global approach to classification.\n",
    "### <u>2.3:</u> A larger gamma value makes the decision boundary focus on closer data points. It leads to a more complex and localized decision boundary that can fit intricate patterns in the data.\n",
    "### <u>2.4:</u> High values of gamma can result in overfitting, as the model becomes too sensitive to individual training examples.\n",
    "### <u>2.5:</u> The optimal gamma value depends on the dataset's characteristics, such as the scale of the features and the level of complexity in the data. It often requires experimentation to find the appropriate value.\n",
    "### –Finding the right values for C and gamma is typically achieved through hyperparameter tuning techniques, such as grid search or random search, where different combinations of values are evaluated to determine the optimal settings for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaca4c0-c769-49e0-bffa-347cea44dfb3",
   "metadata": {},
   "source": [
    "## •<u>GridSearch CV – to select the optimum hyperparameters</u>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc187c8a-4dfb-453c-b585-d3d0b8a70bb4",
   "metadata": {},
   "source": [
    "### – GridSearchCV is a popular method for hyperparameter tuning in machine learning, including for Support Vector Machines (SVMs). It is a systematic approach that exhaustively searches through a predefined grid of hyperparameter values to find the optimal combination that yields the best performance.\n",
    "### – Here's a step-by-step overview of how GridSearchCV works for selecting the optimum hyperparameters in SVM:\n",
    "### 1. <u>Define the parameter grid</u> \n",
    "### 2. <u>Create the SVM model</u> \n",
    "### 3. <u>Set up the GridSearchCV</u> \n",
    "### 4. <u>Perform grid search</u> \n",
    "### 5. <u>Find the best hyperparameters</u> \n",
    "### 6. <u>Evaluate the model</u> \n",
    "### – By performing a grid search using cross-validation, GridSearchCV helps in finding the hyperparameter values that optimize the model's performance while avoiding overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a306f5-912a-4821-a89b-f827b715946d",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aa4d3-2812-4ce0-94ca-8ef91d157e37",
   "metadata": {},
   "source": [
    "#### •<u>Hyperparameter tuning with GridSearchCV:</u> GridSearchCV is employed for hyperparameter tuning. A parameter grid with different combinations of the cost (C) and gamma values is defined using 'param_grid'. GridSearchCV is initialized with the SVM model, the parameter grid, and verbosity. The 'grid.fit(X_train, y_train)' method is called to perform the grid search and find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55435e65-3600-4ad4-905b-89d66b4ceb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1195e64b-d0d2-4a18-a57f-3c6cddacc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5ab6bab-5939-411d-97c9-c3d703cda599",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(), param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ec1b0ec-dfc6-472a-af3a-262ce58c0c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5] END ....................C=0.1, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ....................C=0.1, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ....................C=0.1, gamma=1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ....................C=0.1, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ....................C=0.1, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ..................C=0.1, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ..................C=0.1, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ..................C=0.1, gamma=0.1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ..................C=0.1, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ..................C=0.1, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .................C=0.1, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .................C=0.1, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .................C=0.1, gamma=0.01;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .................C=0.1, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .................C=0.1, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ................C=0.1, gamma=0.001;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ................C=0.1, gamma=0.001;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ................C=0.1, gamma=0.001;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ................C=0.1, gamma=0.001;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ................C=0.1, gamma=0.001;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ...............C=0.1, gamma=0.0001;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END ...............C=0.1, gamma=0.0001;, score=0.938 total time=   0.0s\n",
      "[CV 3/5] END ...............C=0.1, gamma=0.0001;, score=0.963 total time=   0.0s\n",
      "[CV 4/5] END ...............C=0.1, gamma=0.0001;, score=0.962 total time=   0.0s\n",
      "[CV 5/5] END ...............C=0.1, gamma=0.0001;, score=0.886 total time=   0.0s\n",
      "[CV 1/5] END ......................C=1, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ......................C=1, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ......................C=1, gamma=1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ......................C=1, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ......................C=1, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ....................C=1, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ....................C=1, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ....................C=1, gamma=0.1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ....................C=1, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ....................C=1, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ...................C=1, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ...................C=1, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ...................C=1, gamma=0.01;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ...................C=1, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ...................C=1, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ..................C=1, gamma=0.001;, score=0.900 total time=   0.0s\n",
      "[CV 2/5] END ..................C=1, gamma=0.001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ..................C=1, gamma=0.001;, score=0.925 total time=   0.0s\n",
      "[CV 4/5] END ..................C=1, gamma=0.001;, score=0.962 total time=   0.0s\n",
      "[CV 5/5] END ..................C=1, gamma=0.001;, score=0.937 total time=   0.0s\n",
      "[CV 1/5] END .................C=1, gamma=0.0001;, score=0.912 total time=   0.0s\n",
      "[CV 2/5] END .................C=1, gamma=0.0001;, score=0.950 total time=   0.0s\n",
      "[CV 3/5] END .................C=1, gamma=0.0001;, score=0.975 total time=   0.0s\n",
      "[CV 4/5] END .................C=1, gamma=0.0001;, score=0.962 total time=   0.0s\n",
      "[CV 5/5] END .................C=1, gamma=0.0001;, score=0.937 total time=   0.0s\n",
      "[CV 1/5] END .....................C=10, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .....................C=10, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .....................C=10, gamma=1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .....................C=10, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .....................C=10, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ...................C=10, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ...................C=10, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ...................C=10, gamma=0.1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ...................C=10, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ...................C=10, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ..................C=10, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ..................C=10, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ..................C=10, gamma=0.01;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END ..................C=10, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ..................C=10, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .................C=10, gamma=0.001;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END .................C=10, gamma=0.001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END .................C=10, gamma=0.001;, score=0.900 total time=   0.0s\n",
      "[CV 4/5] END .................C=10, gamma=0.001;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END .................C=10, gamma=0.001;, score=0.924 total time=   0.0s\n",
      "[CV 1/5] END ................C=10, gamma=0.0001;, score=0.950 total time=   0.0s\n",
      "[CV 2/5] END ................C=10, gamma=0.0001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ................C=10, gamma=0.0001;, score=0.975 total time=   0.0s\n",
      "[CV 4/5] END ................C=10, gamma=0.0001;, score=0.949 total time=   0.0s\n",
      "[CV 5/5] END ................C=10, gamma=0.0001;, score=0.949 total time=   0.0s\n",
      "[CV 1/5] END ....................C=100, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ....................C=100, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ....................C=100, gamma=1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ....................C=100, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ....................C=100, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ..................C=100, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ..................C=100, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ..................C=100, gamma=0.1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ..................C=100, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ..................C=100, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .................C=100, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .................C=100, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .................C=100, gamma=0.01;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END .................C=100, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .................C=100, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ................C=100, gamma=0.001;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END ................C=100, gamma=0.001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ................C=100, gamma=0.001;, score=0.900 total time=   0.0s\n",
      "[CV 4/5] END ................C=100, gamma=0.001;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END ................C=100, gamma=0.001;, score=0.924 total time=   0.0s\n",
      "[CV 1/5] END ...............C=100, gamma=0.0001;, score=0.925 total time=   0.0s\n",
      "[CV 2/5] END ...............C=100, gamma=0.0001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ...............C=100, gamma=0.0001;, score=0.975 total time=   0.0s\n",
      "[CV 4/5] END ...............C=100, gamma=0.0001;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END ...............C=100, gamma=0.0001;, score=0.949 total time=   0.0s\n",
      "[CV 1/5] END ...................C=1000, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ...................C=1000, gamma=1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ...................C=1000, gamma=1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END ...................C=1000, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ...................C=1000, gamma=1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END .................C=1000, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END .................C=1000, gamma=0.1;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END .................C=1000, gamma=0.1;, score=0.625 total time=   0.0s\n",
      "[CV 4/5] END .................C=1000, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END .................C=1000, gamma=0.1;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ................C=1000, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 2/5] END ................C=1000, gamma=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 3/5] END ................C=1000, gamma=0.01;, score=0.613 total time=   0.0s\n",
      "[CV 4/5] END ................C=1000, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END ................C=1000, gamma=0.01;, score=0.633 total time=   0.0s\n",
      "[CV 1/5] END ...............C=1000, gamma=0.001;, score=0.887 total time=   0.0s\n",
      "[CV 2/5] END ...............C=1000, gamma=0.001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ...............C=1000, gamma=0.001;, score=0.900 total time=   0.0s\n",
      "[CV 4/5] END ...............C=1000, gamma=0.001;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END ...............C=1000, gamma=0.001;, score=0.924 total time=   0.0s\n",
      "[CV 1/5] END ..............C=1000, gamma=0.0001;, score=0.938 total time=   0.0s\n",
      "[CV 2/5] END ..............C=1000, gamma=0.0001;, score=0.912 total time=   0.0s\n",
      "[CV 3/5] END ..............C=1000, gamma=0.0001;, score=0.963 total time=   0.0s\n",
      "[CV 4/5] END ..............C=1000, gamma=0.0001;, score=0.924 total time=   0.0s\n",
      "[CV 5/5] END ..............C=1000, gamma=0.0001;, score=0.962 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093dd80-1bb2-4aaa-8510-b470d4dbc03c",
   "metadata": {},
   "source": [
    "#### •<u>Evaluating the tuned model:</u> The best hyperparameters are accessed using 'grid.best_params_', and the best estimator is obtained using 'grid.best_estimator_'. Predictions are made on the test data using the tuned model ('grid_predictions'). The confusion matrix and classification report are printed again to evaluate the tuned model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a119d1e-c1aa-4ae9-bdbd-2d56509ddab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.0001}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8157f6d0-d550-4e80-af22-a79f076e42b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.0001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3daacf1f-b5a8-4b38-8a01-2e21bce8fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebaaf984-6e0f-4d39-a08a-52cb05c06dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59   7]\n",
      " [  4 101]]\n",
      "__________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91        66\n",
      "           1       0.94      0.96      0.95       105\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.93       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, grid_predictions))\n",
    "print(50*'_')\n",
    "print(classification_report(y_test, grid_predictions))\n",
    "print(50*'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20c76c-75f1-4853-aedd-db68200acba0",
   "metadata": {},
   "source": [
    "### •<u>Confusion Matrix Explained (post tuning)</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b55f5-e145-472d-a33d-68d308d194f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <u>[1,1] True Negative (TN):</u> The model correctly predicted 59 samples as negative (benign tumors).\n",
    "#### <u>[1,2] False Positive (FP):</u> The model incorrectly predicted 7 samples as positive (malignant tumors) when they were actually negative (benign tumors).\n",
    "#### <u>[2,1] False Negative (FN):</u> The model incorrectly predicted 4 samples as negative (benign tumors) when they were actually positive (malignant tumors).\n",
    "#### <u>[2,2] True Positive (TP):</u> The model correctly predicted 101 samples as positive (malignant tumors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd94a7f-38f0-4efe-ab6e-7390893874f8",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd5b52-9a8f-43f2-b40c-280dd72acbdc",
   "metadata": {},
   "source": [
    "## •<u>Advantages of the above SVM Model</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68174473-304e-46bb-a716-ce3fabc35c1b",
   "metadata": {},
   "source": [
    "### 1 \t\t<u>Effective in High-Dimensional Spaces:</u> SVMs perform well in datasets with a high number of features or dimensions. They can handle and make accurate predictions in datasets where the number of features is greater than the number of samples.\n",
    "### 2 \t\t<u>Versatile Kernel Functions:</u> SVMs can utilize different kernel functions, such as linear, polynomial, and radial basis function (RBF), to handle both linearly separable and non-linearly separable data. This flexibility allows SVMs to capture complex patterns and relationships in the data.\n",
    "### 3 \t\t<u>Robust to Overfitting:</u> SVMs have a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing the classification error. This regularization helps prevent overfitting by balancing the model's ability to fit the training data while generalizing well to unseen data.\n",
    "### 4 \t\t<u>Effective for Small/Medium-Sized Datasets:</u> SVMs are particularly suitable for datasets with a small to medium number of samples. They can provide accurate and stable predictions even with limited training data.\n",
    "### 5 \t\t<u>Global Optimal Solution:</u> The objective of SVMs is to find the hyperplane that maximizes the margin between classes. This optimization problem has a unique global optimal solution, ensuring consistency and stability in model training.\n",
    "### 6 \t\t<u>Robust to Outliers:</u> SVMs are less sensitive to outliers compared to some other classification algorithms. The use of a margin maximization objective helps to ignore outliers that fall within the margin.\n",
    "### 7 \t\t<u>Memory Efficient:</u> SVMs only rely on a subset of training points (support vectors) to define the decision boundary. This property makes SVMs memory efficient and allows them to scale well to large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5778ef-70d3-4afa-bc1b-b6d8def5b9e3",
   "metadata": {},
   "source": [
    "## •<u>Conclusion</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16304e13-fec3-4d5c-831b-8330d435eccb",
   "metadata": {},
   "source": [
    "### The code implements an SVM model for the classification of breast cancer data. It demonstrates the training and evaluation of the model, including the use of confusion matrices and classification reports to assess its performance. It also showcases the use of grid search for hyperparameter tuning. \n",
    "### It hence proves that hyperparameter tuning is therefore important as it gave a better result after its implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d189a94-a0fa-474b-b988-e0e412d12dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
